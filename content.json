{"meta":{"title":"璃月のParodist","subtitle":"♪ ♩  Blog-Ver1.0 ♫ ♬","description":"力量若是达到极限，接下来考验的便是人心","author":"Parodist1225","url":"https://Parodist1225.github.io","root":"/"},"pages":[{"title":"标签","date":"2023-03-15T12:36:02.000Z","updated":"2023-06-05T16:45:09.019Z","comments":true,"path":"tags/index.html","permalink":"https://parodist1225.github.io/tags/index.html","excerpt":"","text":""},{"title":"分类","date":"2023-03-15T12:36:37.000Z","updated":"2023-06-05T16:45:44.753Z","comments":true,"path":"categories/index.html","permalink":"https://parodist1225.github.io/categories/index.html","excerpt":"","text":""},{"title":"model","date":"2023-03-15T08:52:46.000Z","updated":"2023-03-15T12:35:25.234Z","comments":true,"path":"model/index.html","permalink":"https://parodist1225.github.io/model/index.html","excerpt":"","text":""},{"title":"about","date":"2023-03-15T12:36:13.000Z","updated":"2023-03-15T12:36:13.927Z","comments":true,"path":"about/index.html","permalink":"https://parodist1225.github.io/about/index.html","excerpt":"","text":""},{"title":"","date":"2023-06-05T10:10:36.930Z","updated":"2023-06-05T10:10:36.930Z","comments":true,"path":"static/css/loading_bar.css","permalink":"https://parodist1225.github.io/static/css/loading_bar.css","excerpt":"","text":".pace { -webkit-pointer-events: none; pointer-events: none; -webkit-user-select: none; -moz-user-select: none; user-select: none; z-index: 2000; position: fixed; margin: auto; top: 10px; left: 0; right: 0; height: 8px; border-radius: 8px; width: 4rem; background: #eaecf2; border: 1px #e3e8f7; overflow: hidden; } .pace-inactive .pace-progress { opacity: 0; transition: 0.3s ease-in; } .pace .pace-progress { -webkit-box-sizing: border-box; -moz-box-sizing: border-box; -ms-box-sizing: border-box; -o-box-sizing: border-box; box-sizing: border-box; -webkit-transform: translate3d(0, 0, 0); -moz-transform: translate3d(0, 0, 0); -ms-transform: translate3d(0, 0, 0); -o-transform: translate3d(0, 0, 0); transform: translate3d(0, 0, 0); max-width: 200px; position: absolute; z-index: 2000; display: block; top: 0; right: 100%; height: 100%; width: 100%; background: linear-gradient(-45deg, #ee7752, #e73c7e, #23a6d5, #23d5ab); animation: gradient 1.5s ease infinite; background-size: 200%; } .pace.pace-inactive { opacity: 0; transition: 0.3s; top: -8px; } @keyframes gradient { 0% { background-position: 0% 50%; } 50% { background-position: 100% 50%; } 100% { background-position: 0% 50%; } }"},{"title":"","date":"2023-06-05T10:22:04.744Z","updated":"2023-06-05T10:22:04.744Z","comments":true,"path":"static/css/loading_gif.css","permalink":"https://parodist1225.github.io/static/css/loading_gif.css","excerpt":"","text":".loading-img { background: url(https://npm.elemecdn.com/anzhiyu-blog@2.1.1/img/avatar.webp) no-repeat center center; background-size: cover; }"}],"posts":[{"title":"添加网页BGM全局吸底效果及踩坑过程","slug":"hexo-init1","date":"2023-06-05T07:34:22.823Z","updated":"2023-06-05T16:49:51.253Z","comments":true,"path":"2023/06/05/hexo-init1/","link":"","permalink":"https://parodist1225.github.io/2023/06/05/hexo-init1/","excerpt":"","text":"总体思路： 采用Aplayer hexo-tag-aplayer插件也能用，警告其中支持的metingjs的网易云api是经常挂掉ψ(*｀ー´)ψ(至少我配置那段时间网易云api是挂了),该方法参考Butterfly添加全局吸底Aplayer教程 在官网下载源码：MoePlayer/APlayer 上述源码只需取dist文件夹，将其解压至Hexo\\themes\\你的主题\\source中 新建Hexo\\themes\\你的主题\\source\\dist\\music.js 内容为（参考） const ap = new APlayer(&#123; container: document.getElementById(&#39;aplayer&#39;), fixed: true, order: &#39;random&#39;, autoplay: true, audio: [ &#123; name: &quot;千年之羽（符华印象曲）&quot;, artist: &#39;小林未郁&#39;, url: &#39;http://parodist1225.top/static/music/小林未郁-千年之羽.mp3&#39;, cover: &#39;http://parodist1225.top/static/music/千年之羽.jpg&#39;, &#125;, &#123; name: &#39;Moon Halo&#39;, artist: &#39;茶理理,TetraCalyx,Hanser&#39;, url: &#39;http://parodist1225.top/static/music/MoonHalo.mp3&#39;, cover: &#39;http://192.168.111.242:8081/IXP46078c365a6cfdefa7c28f4479e73221179b16fae0c64_10.11.178.133/p1.music.126.net/ciLKATqflV2YWSV3ltE2Kw==/109951166159281275.jpg?param=130y130&#39;, &#125;, &#123; name: &#39;Scarborough Fair&#39;, artist: &#39;山田タマル&#39;, url: &#39;http://music.163.com/song/media/outer/url?id=489970551.mp3&#39;, cover: &#39;https://p1.music.126.net/PFu_Fb_sYULwkoJ87mHDmA==/19023750184014421.jpg?param=90y90&#39;, &#125;, &#123; name: &#39;I Really Want to Stay at Your House&#39;, artist: &#39;Samuel Kim / Lorien&#39;, url: &#39;http://music.163.com/song/media/outer/url?id=1990743306.mp3&#39;, cover: &#39;https://p2.music.126.net/-LHyMtXmo-yDf-eqwb8ThA==/109951168143056350.jpg?param=90y90&#39;, &#125;, ] &#125;); url如网易云样式http://music.163.com/song/media/outer/url?id=歌曲ID.mp3 更多参数参考上述官方文档Aplayer 这一步来实现全网页的添加，思路是将下列代码添加至博客每个页面底部以Butterfly主题为例，取该主题_config.butterfly.yml文件中inject部分，将下列四行代码始终挂载在各页面底部inject: head: # - &lt;link rel=&quot;stylesheet&quot; href=&quot;/xxx.css&quot;&gt; bottom: #- &lt;div class=&quot;aplayer no-destroy&quot; data-id=&quot;60198&quot; data-server=&quot;netease&quot; data-type=&quot;playlist&quot; data-fixed=&quot;true&quot; data-autoplay=&quot;true&quot;&gt; &lt;/div&gt; - &lt;link rel=&quot;stylesheet&quot; href=&quot;/dist/APlayer.min.css&quot;&gt; - &lt;div id=&quot;aplayer&quot;&gt;&lt;/div&gt; - &lt;script type=&quot;text/javascript&quot; src=&quot;/dist/APlayer.min.js&quot;&gt;&lt;/script&gt; - &lt;script type=&quot;text/javascript&quot; src=&quot;/dist/music.js&quot;&gt;&lt;/script&gt;","categories":[{"name":"前端","slug":"前端","permalink":"https://parodist1225.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://parodist1225.github.io/tags/hexo/"}]},{"title":"解决hexo-asset-image插件Bug:引用本地资源后路径中出现/.io/或者/.com/","slug":"hexo-bug1","date":"2023-06-04T15:24:53.401Z","updated":"2023-06-05T16:49:57.540Z","comments":true,"path":"2023/06/04/hexo-bug1/","link":"","permalink":"https://parodist1225.github.io/2023/06/04/hexo-bug1/","excerpt":"","text":"问题描述：在上传文章中用markdown语法如![](img.jpg)时在实际网站中GET的路径自行被添加/.io/导致获取不到该图片 解决方法：打开\\blog(根目录)\\node_modules\\hexo-asset-image\\index.js，将原第24行var endPos = link.lastIndexOf(&#39;.&#39;);改为var endPos = link.length-1;","categories":[{"name":"前端","slug":"前端","permalink":"https://parodist1225.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Debug","slug":"Debug","permalink":"https://parodist1225.github.io/tags/Debug/"}]},{"title":"神经网络入门1","slug":"networkbase1","date":"2023-06-03T11:43:46.038Z","updated":"2023-06-05T16:52:47.034Z","comments":true,"path":"2023/06/03/networkbase1/","link":"","permalink":"https://parodist1225.github.io/2023/06/03/networkbase1/","excerpt":"","text":"原文出处Machine Learning for Beginners: An Introduction to Neural Networks 砖块：神经元神经网络的基本单位，神经元。神经元接受输入，对其做一些数据操作，然后产生输出。例如，这是一个2-输入神经元：这里发生了三个事情。首先，每个输入都跟一个权重相乘（红色）： x_1→x_1*w_1 \\\\ x_2→x_2*w_2然后，加权后的输入求和，加上一个偏差b（绿色）： (x_1*w_1)+(x_2*w_2)+b最后，这个结果传递给一个激活函数f： y=f(x_1*w_1+x_2*w_2+b)激活函数的用途是将一个无边界的输入，转变成一个可预测的形式。常用的激活函数就就是S型函数：S型函数的值域是$(0, 1)$。简单来说，就是把$(−∞, +∞)$压缩到$(0, 1)$ ，很大的负数约等于$0$，很大的正数约等于$1$。 一个简单的例子假设我们有一个2-输入神经元，激活函数就是S型函数，其参数如下： w=[0,1] \\\\ b=4$w=[0,1]$就是以向量的形式表示$w_1=0, w_2=1$。现在，我们给这个神经元一个输入$x=[2, 3]$。我们用点积来表示： 当输入是[2, 3]时，这个神经元的输出是0.999。给定输入，得到输出的过程被称为前馈（feedforward）。 编码一个神经元让我们来实现一个神经元！用Python的NumPy库来完成其中的数学计算： import numpy as np def sigmoid(x): # Our activation function: f(x) = 1 / (1 + e^(-x)) return 1 / (1 + np.exp(-x)) class Neuron: def __init__(self, weights, bias): self.weights = weights self.bias = bias def feedforward(self, inputs): # Weight inputs, add bias, then use the activation function total = np.dot(self.weights, inputs) + self.bias return sigmoid(total) weights = np.array([0, 1]) # w1 = 0, w2 = 1 bias = 4 # b = 4 n = Neuron(weights, bias) x = np.array([2, 3]) # x1 = 2, x2 = 3 print(n.feedforward(x)) # 0.9990889488055994 还记得这个数字吗？就是我们前面算出来的例子中的0.999。 把神经元组装成网络所谓的神经网络就是一堆神经元。这就是一个简单的神经网络：这个网络有两个输入，一个有两个神经元（$h_1$和$h_2$）的隐藏层，以及一个有一个神经元（$o_1$）的输出层。要注意,$o_1$的输入就是 $h_1$和 $h_2$的输出，这样就组成了一个网络。 隐藏层就是输入层和输出层之间的层，隐藏层可以是多层的。 例子：前馈我们继续用前面图中的网络，假设每个神经元的权重都是$w=[0,1]$ ，截距项也相同 $b=0$，激活函数也都是S型函数。分别用 $h_1,h_2,o_1$表示相应的神经元的输出。 当输入$x=[2,3]$时，会得到什么结果？这个神经网络对输入[2,3]的输出是0.7216，很简单。 一个神经网络的层数以及每一层中的神经元数量都是任意的。基本逻辑都一样：输入在神经网络中向前传输，最终得到输出。接下来，我们会继续使用前面的这个网络。 编码神经网络：前馈接下来我们实现这个神经网络的前馈机制，还是这个图： import numpy as np # ... code from previous section here class OurNeuralNetwork: &#39;&#39;&#39; A neural network with: - 2 inputs - a hidden layer with 2 neurons (h1, h2) - an output layer with 1 neuron (o1) Each neuron has the same weights and bias: - w = [0, 1] - b = 0 &#39;&#39;&#39; def __init__(self): weights = np.array([0, 1]) bias = 0 # The Neuron class here is from the previous section self.h1 = Neuron(weights, bias) self.h2 = Neuron(weights, bias) self.o1 = Neuron(weights, bias) def feedforward(self, x): out_h1 = self.h1.feedforward(x) out_h2 = self.h2.feedforward(x) # The inputs for o1 are the outputs from h1 and h2 out_o1 = self.o1.feedforward(np.array([out_h1, out_h2])) return out_o1 network = OurNeuralNetwork() x = np.array([2, 3]) print(network.feedforward(x)) # 0.7216325609518421 We got 0.7216 again! Looks like it works. 训练神经网络，第1部分现在有这样的数据： Name Weight(lb) Height(in) Gender Alice 133 65 F Bob 160 72 M Charlie 152 70 M Diana 120 60 F 接下来我们用这个数据来训练神经网络的权重和截距项，从而可以根据身高体重预测性别：我们用0和1分别表示男性（M）和女性（F），并对数值做了转化： Name Weight(minus 135) Height(minus 66) Gender Alice -2 -1 1 Bob 25 6 0 Charlie 17 4 0 Diana -15 -6 1 我这里是随意选取了135和66来标准化数据，通常会使用平均值。 损失在训练网络之前，我们需要量化当前的网络是『好』还是『坏』，从而可以寻找更好的网络。这就是定义损失的目的。 我们在这里用平均方差（MSE）损失：$MSE=\\frac{1}{n}\\sum_{i=1}^{n} (y_{true}-y_{pred})^2$ 让我们仔细看看： n是样品数，这里等于4（Alice、Bob、Charlie和Diana）。 y表示要预测的变量，这里是性别。 $y_{true}$ 是变量的真实值（『正确答案』）。例如，Alice的$y_{true}$ 就是1（男性）。 $y_{pred}$ 是变量的预测值。这就是我们网络的输出。 $(y_{true}-y_{pred})^2$被称为方差（squared error）。我们的损失函数就是所有方差的平均值。预测效果于浩，损失就越少。更好的预测 = 更少的损失！训练网络 = 最小化它的损失。 损失计算例子假设我们的网络总是输出0，换言之就是认为所有人都是男性。损失如何？🤔 代码：MSE损失下面是计算MSE损失的代码： import numpy as np def mse_loss(y_true, y_pred): # y_true and y_pred are numpy arrays of the same length. return ((y_true - y_pred) ** 2).mean() y_true = np.array([1, 0, 0, 1]) y_pred = np.array([0, 0, 0, 0]) print(mse_loss(y_true, y_pred)) # 0.5 如果你不理解这段代码，可以看看NumPy的快速入门中关于数组的操作。 Nice. Onwards! 训练神经网络，第2部分现在我们有了一个明确的目标：最小化神经网络的损失。通过调整网络的权重和截距项，我们可以改变其预测结果，但如何才能逐步地减少损失？ 这一段内容涉及到多元微积分，如果不熟悉微积分的话，可以跳过这些数学内容。 为了简化问题，假设我们的数据集中只有Alice： Name Weight(minus 135) Height(minus 66) Gender Alice -2 -1 1 Name $y_{true}$ $y_{pred}$ $(y_{true}-y_{pred})^2$ Alice 1 0 1 那均方差损失就只是Alice的方差：$MSE=\\frac{1}{1}\\sum_{i=1}^{1} (y_{true}-y_{pred})^2 \\\\\\quad \\quad= (y_{true}-y_{pred})^2 \\\\\\quad \\quad= (1-y_{pred})^2$也可以把损失看成是权重和截距项的函数。让我们给网络标上权重和截距项：.svg)这样我们就可以把网络的损失表示为： L(w_1,w_2,w_3,w_4,w_5,w_6,b_1,b_2,b_3)假设我们要优化 $w_1$，当我们改变$w_1$时，损失 $L$ 会怎么变化？可以用 $ \\frac{\\partial L}{\\partial w_1}$来回答这个问题，怎么计算？ 接下来的数据稍微有点复杂，别担心，准备好纸和笔。 首先，让我们用 $ \\frac{\\partial y_{pred}}{\\partial w_1}$来改写这个偏导数：这种计算偏导的方法叫『反向传播算法』(backpropagation)。 好多数学符号，如果你还没搞明白的话，我们来看一个实际例子。 例子：计算偏导数我们还是看数据集中只有Alice的情况: Name Weight(minus 135) Height(minus 66) Gender Alice -2 -1 1 Name $y_{true}$ $y_{pred}$ $(y_{true}-y_{pred})^2$ Alice 1 0 1 把所有的权重和截距项都分别初始化为1和0。在网络中做前馈计算： 训练：随机梯度下降现在训练神经网络已经万事俱备了！我们会使用名为随机梯度下降法的优化算法来优化网络的权重和截距项，实现损失的最小化。核心就是这个更新等式： w_1←w_1-η\\frac{\\partial L}{\\partial w_1} η是一个常数，被称为学习率，用于调整训练的速度。我们要做的就是用$w_1$ 减去$η\\frac{\\partial L}{\\partial w_1}$ ： 如果 $\\frac{\\partial L}{\\partial w_1}$是正数，$w_1$ 会变小，$L$ 会下降。 如果 $\\frac{\\partial L}{\\partial w_1}$是负数，$w_1$ 会变大，$L$ 会下降。如果我们对网络中的每个权重和截距项都这样进行优化，损失就会不断下降，网络性能会不断上升。 我们的训练过程是这样的： 从我们的数据集中选择一个样本，用随机梯度下降法进行优化——每次我们都只针对一个样本进行优化； 计算每个权重或截距项对损失的偏导（例如 $\\frac{\\partial L}{\\partial w_1}$ 、 $\\frac{\\partial L}{\\partial w_2}$ 等）； 用更新等式更新每个权重和截距项； 重复第一步； 代码：一个完整的神经网络我们终于可以实现一个完整的神经网络了： Name Weight(minus 135) Height(minus 66) Gender Alice -2 -1 1 Bob 25 6 0 Charlie 17 4 0 Diana -15 -6 1 import numpy as np def sigmoid(x): # Sigmoid activation function: f(x) = 1 / (1 + e^(-x)) return 1 / (1 + np.exp(-x)) def deriv_sigmoid(x): # Derivative of sigmoid: f&#39;(x) = f(x) * (1 - f(x)) fx = sigmoid(x) return fx * (1 - fx) def mse_loss(y_true, y_pred): # y_true and y_pred are numpy arrays of the same length. return ((y_true - y_pred) ** 2).mean() class OurNeuralNetwork: &#39;&#39;&#39; A neural network with: - 2 inputs - a hidden layer with 2 neurons (h1, h2) - an output layer with 1 neuron (o1) *** DISCLAIMER ***: The code below is intended to be simple and educational, NOT optimal. Real neural net code looks nothing like this. DO NOT use this code. Instead, read/run it to understand how this specific network works. &#39;&#39;&#39; def __init__(self): # Weights self.w1 = np.random.normal() self.w2 = np.random.normal() self.w3 = np.random.normal() self.w4 = np.random.normal() self.w5 = np.random.normal() self.w6 = np.random.normal() # Biases self.b1 = np.random.normal() self.b2 = np.random.normal() self.b3 = np.random.normal() def feedforward(self, x): # x is a numpy array with 2 elements. h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1) h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2) o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3) return o1 def train(self, data, all_y_trues): &#39;&#39;&#39; - data is a (n x 2) numpy array, n = # of samples in the dataset. - all_y_trues is a numpy array with n elements. Elements in all_y_trues correspond to those in data. &#39;&#39;&#39; learn_rate = 0.1 epochs = 1000 # number of times to loop through the entire dataset for epoch in range(epochs): for x, y_true in zip(data, all_y_trues): # --- Do a feedforward (we&#39;ll need these values later) sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1 h1 = sigmoid(sum_h1) sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2 h2 = sigmoid(sum_h2) sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3 o1 = sigmoid(sum_o1) y_pred = o1 # --- Calculate partial derivatives. # --- Naming: d_L_d_w1 represents &quot;partial L / partial w1&quot; d_L_d_ypred = -2 * (y_true - y_pred) # Neuron o1 d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1) d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1) d_ypred_d_b3 = deriv_sigmoid(sum_o1) d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1) d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1) # Neuron h1 d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1) d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1) d_h1_d_b1 = deriv_sigmoid(sum_h1) # Neuron h2 d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2) d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2) d_h2_d_b2 = deriv_sigmoid(sum_h2) # --- Update weights and biases # Neuron h1 self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1 self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2 self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1 # Neuron h2 self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3 self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4 self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2 # Neuron o1 self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5 self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6 self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3 # --- Calculate total loss at the end of each epoch if epoch % 10 == 0: y_preds = np.apply_along_axis(self.feedforward, 1, data) loss = mse_loss(all_y_trues, y_preds) print(&quot;Epoch %d loss: %.3f&quot; % (epoch, loss)) # Define dataset data = np.array([ [-2, -1], # Alice [25, 6], # Bob [17, 4], # Charlie [-15, -6], # Diana ]) all_y_trues = np.array([ 1, # Alice 0, # Bob 0, # Charlie 1, # Diana ]) # Train our neural network! network = OurNeuralNetwork() network.train(data, all_y_trues) You can run / play with this code yourself. It’s also available on Github. 随着网络的学习，损失在稳步下降。现在我们可以用这个网络来预测性别了： # Make some predictions emily = np.array([-7, -3]) # 128 pounds, 63 inches frank = np.array([20, 2]) # 155 pounds, 68 inches print(&quot;Emily: %.3f&quot; % network.feedforward(emily)) # 0.951 - F print(&quot;Frank: %.3f&quot; % network.feedforward(frank)) # 0.039 - M 接下来？搞定了一个简单的神经网络，快速回顾一下： 介绍了神经网络的基本结构——神经元； 在神经元中使用S型激活函数； 神经网络就是连接在一起的神经元； 构建了一个数据集，输入（或特征）是体重和身高，输出（或标签）是性别； 学习了损失函数和均方差损失； 训练网络就是最小化其损失； 用反向传播方法计算偏导； 用随机梯度下降法训练网络； 接下来你还可以：跳转底部链接","categories":[{"name":"AI相关","slug":"AI相关","permalink":"https://parodist1225.github.io/categories/AI%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"机器学习","slug":"机器学习","permalink":"https://parodist1225.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"bp神经网络","slug":"bp神经网络","permalink":"https://parodist1225.github.io/tags/bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"}]},{"title":"TS(一)","slug":"test","date":"2022-09-15T10:15:53.230Z","updated":"2023-06-05T16:48:48.698Z","comments":true,"path":"2022/09/15/test/","link":"","permalink":"https://parodist1225.github.io/2022/09/15/test/","excerpt":"","text":"typescript与Javascriptjs是动态类型弱类型语言，而ts是静态类型弱类型语言。强类型语言：不允许改变变量的数据类型，除非进行强制类型转换。如python弱类型语言：变量可以被赋予不同的值。静态类型语言：在编译阶段确定所有变量的类型。动态类型语言：在执行阶段确定所有变量的类型。 ts的数据类型ts的数据类型有两种，一种是js原有的数据类型，一种是新增的数据类型原有数据类型原始类型：number/string/bollean/null/undefined/symbol对象类型：object（数组、对象、函数等）js写法: let a = &#39;hhh&#39; let b = 666 let c = true let d = null let e = undefined let f = &#123;name:&#39;maobuhui&#39;&#125; let g = 100 let h = Symbol() ts写法: let a:string = &#39;hhh&#39; let b:number = 666 let c:boolean = true let d:null = null let e:undefined = undefined let f:object = &#123;name:&#39;maobuhui&#39;&#125; let g:bigint = 100n let h:symbol = Symbol() 新增类型 联合类型 自定义类型(类型别名) 接口 元组 字面量类型 枚举 void any对象类型(接口 interface)```jsinterface IByteDancer{ /只读属性：约束属性不可在对象初始化外赋值/ readonly jobId:number; name:string; sex:’man’|’woman’|’other’; age:number; /可选属性：定义该属性可以不存在/ hobby?:string; /任意属性：约束所有对象属性必须是该属性的子类型/ [key:string]:any; }const bytedancer: IBytedancer = { jobId:9303245, name: ‘lin’, sex: ‘man’, age: 28, hobby: ‘swimming’} 函数类型 ```ts function add(x:number,y:number):number&#123; return x+y; &#125; const mult:(x:number,y:number) =&gt; number = (x,y) =&gt; x*y; 函数重载 /*对getDate函数进行重载，timestamp为可缺省参数*/ function getDate(type:&#39;string&#39;, timestamp?: string) :string; function getDate (type:&#39;date&#39; ,timestamp?: string): Date; function getDate(type: &#39;string&#39; | &#39;date&#39;, timestamp?: string): Date | string &#123; const date = new Date(timestamp); return type === &#39;string&#39; ? date.toLocaleString() : date; &#125; const X = getDate( &#39;date&#39;); // x: Date const y = getDate(&#39;string&#39;, &#39;2018-01-10&#39;); // y: string interface IGetDate &#123; (type :&#39;string&#39;, timestamp?: string): string; (type :&#39;date&#39; , timestamp?: string): Date ; (type : &#39;string&#39; | &#39;date&#39;,timestamp?: string): Date | string; &#125; /*不能将类型“(type:any, timestamp: any) =&gt; string | Date&quot; 分配给类型&quot;IGetDate&quot;。 不能将类型&quot;string | Date&quot; 分配给类型&quot;string&quot;。 不能将类型&quot;Date&quot;分配给类型&quot;string&quot;。ts (2322) */ const getDate2: IGetDate = (type, timestamp) =&gt; &#123; const date = new Date( timestamp ) ; return type === &#39;string&#39; ? date. toLocaleString() : date; &#125; 数组类型 /* 「类型+方括号」表示*/ type IArr1 = number[]; /*泛型表示 */ type IArr2 = Array&lt;string|number| Record&lt;string, number&gt;&gt;; /*元祖表示*/ type IArr3 = [number, number, string, string] ; /*接口表示*/ interface IArr4 &#123; [key: number]: any; &#125; const arr1: IArr1 = [1 , 2, 3, 4, 5, 6]; const arr2: IArr2 = [1 , 2, &#39;3&#39;, &#39;4&#39;,&#123;a:1&#125;]; const arr3: IArr3 = [1, 2, &#39;3&#39;, &#39;4&#39;]; const arr4: IArr4 = [&#39;string&#39;, () =&gt; null, &#123;&#125;, []]; ts补充类型 /*空类型，表示无赋值*/ type IEmptyFunction = () =&gt; void; /*任意类型，是所有类型的子类型*/ type IAnyType = any ; /*枚举类型:支持枚举值到枚举名的正、反向映射*/ enum EnumExample &#123; add = &#39;+&#39;, mult = &#39;*&#39; &#125; EnumExample[&#39;add&#39;] === &#39;+&#39;; EnumExample[&#39;+&#39;] === &#39;add&#39; ; enum ECorlor &#123; Mon, Tue, Wed, Thu, Fri, Sat, Sun &#125;; ECorlor[&#39;Mon&#39;] === 0; ECorlor[0] === &#39;Mon&#39; ; /*泛型*/ type INumArr = Array&lt;number&gt;;","categories":[{"name":"前端","slug":"前端","permalink":"https://parodist1225.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"Typescipt","slug":"Typescipt","permalink":"https://parodist1225.github.io/tags/Typescipt/"}]},{"title":"Hello World","slug":"hello-world","date":"2022-09-11T11:06:14.912Z","updated":"2023-06-05T16:50:23.989Z","comments":true,"path":"2022/09/11/hello-world/","link":"","permalink":"https://parodist1225.github.io/2022/09/11/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post$ hexo new &quot;My New Post&quot; More info: Writing Run server$ hexo server More info: Server Generate static files$ hexo generate More info: Generating Deploy to remote sites$ hexo deploy More info: Deployment","categories":[{"name":"前端","slug":"前端","permalink":"https://parodist1225.github.io/categories/%E5%89%8D%E7%AB%AF/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://parodist1225.github.io/tags/hexo/"}]}],"categories":[{"name":"前端","slug":"前端","permalink":"https://parodist1225.github.io/categories/%E5%89%8D%E7%AB%AF/"},{"name":"AI相关","slug":"AI相关","permalink":"https://parodist1225.github.io/categories/AI%E7%9B%B8%E5%85%B3/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://parodist1225.github.io/tags/hexo/"},{"name":"Debug","slug":"Debug","permalink":"https://parodist1225.github.io/tags/Debug/"},{"name":"机器学习","slug":"机器学习","permalink":"https://parodist1225.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"bp神经网络","slug":"bp神经网络","permalink":"https://parodist1225.github.io/tags/bp%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"},{"name":"Typescipt","slug":"Typescipt","permalink":"https://parodist1225.github.io/tags/Typescipt/"}]}